{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge  \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data into dataframes\n",
    "train_data = pd.read_csv('train_house.csv',low_memory=False)\n",
    "test_data = pd.read_csv('test_house.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e150561",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "953ae063",
   "metadata": {},
   "source": [
    "Relating what each column represents\n",
    "\n",
    "SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n",
    "MSSubClass: The building class\n",
    "MSZoning: The general zoning classification\n",
    "LotFrontage: Linear feet of street connected to property\n",
    "LotArea: Lot size in square feet\n",
    "Street: Type of road access\n",
    "Alley: Type of alley access\n",
    "LotShape: General shape of property\n",
    "LandContour: Flatness of the property\n",
    "Utilities: Type of utilities available\n",
    "LotConfig: Lot configuration\n",
    "LandSlope: Slope of property\n",
    "Neighborhood: Physical locations within Ames city limits\n",
    "Condition1: Proximity to main road or railroad\n",
    "Condition2: Proximity to main road or railroad (if a second is present)\n",
    "BldgType: Type of dwelling\n",
    "HouseStyle: Style of dwelling\n",
    "OverallQual: Overall material and finish quality\n",
    "OverallCond: Overall condition rating\n",
    "YearBuilt: Original construction date\n",
    "YearRemodAdd: Remodel date\n",
    "RoofStyle: Type of roof\n",
    "RoofMatl: Roof material\n",
    "Exterior1st: Exterior covering on house\n",
    "Exterior2nd: Exterior covering on house (if more than one material)\n",
    "MasVnrType: Masonry veneer type\n",
    "MasVnrArea: Masonry veneer area in square feet\n",
    "ExterQual: Exterior material quality\n",
    "ExterCond: Present condition of the material on the exterior\n",
    "Foundation: Type of foundation\n",
    "BsmtQual: Height of the basement\n",
    "BsmtCond: General condition of the basement\n",
    "BsmtExposure: Walkout or garden level basement walls\n",
    "BsmtFinType1: Quality of basement finished area\n",
    "BsmtFinSF1: Type 1 finished square feet\n",
    "BsmtFinType2: Quality of second finished area (if present)\n",
    "BsmtFinSF2: Type 2 finished square feet\n",
    "BsmtUnfSF: Unfinished square feet of basement area\n",
    "TotalBsmtSF: Total square feet of basement area\n",
    "Heating: Type of heating\n",
    "HeatingQC: Heating quality and condition\n",
    "CentralAir: Central air conditioning\n",
    "Electrical: Electrical system\n",
    "1stFlrSF: First Floor square feet\n",
    "2ndFlrSF: Second floor square feet\n",
    "LowQualFinSF: Low quality finished square feet (all floors)\n",
    "GrLivArea: Above grade (ground) living area square feet\n",
    "BsmtFullBath: Basement full bathrooms\n",
    "BsmtHalfBath: Basement half bathrooms\n",
    "FullBath: Full bathrooms above grade\n",
    "HalfBath: Half baths above grade\n",
    "Bedroom: Number of bedrooms above basement level\n",
    "Kitchen: Number of kitchens\n",
    "KitchenQual: Kitchen quality\n",
    "TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "Functional: Home functionality rating\n",
    "Fireplaces: Number of fireplaces\n",
    "FireplaceQu: Fireplace quality\n",
    "GarageType: Garage location\n",
    "GarageYrBlt: Year garage was built\n",
    "GarageFinish: Interior finish of the garage\n",
    "GarageCars: Size of garage in car capacity\n",
    "GarageArea: Size of garage in square feet\n",
    "GarageQual: Garage quality\n",
    "GarageCond: Garage condition\n",
    "PavedDrive: Paved driveway\n",
    "WoodDeckSF: Wood deck area in square feet\n",
    "OpenPorchSF: Open porch area in square feet\n",
    "EnclosedPorch: Enclosed porch area in square feet\n",
    "3SsnPorch: Three season porch area in square feet\n",
    "ScreenPorch: Screen porch area in square feet\n",
    "PoolArea: Pool area in square feet\n",
    "PoolQC: Pool quality\n",
    "Fence: Fence quality\n",
    "MiscFeature: Miscellaneous feature not covered in other categories\n",
    "MiscVal: Value of miscellaneous feature\n",
    "MoSold: Month Sold\n",
    "YrSold: Year Sold\n",
    "SaleType: Type of sale\n",
    "SaleCondition: Condition of sale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215f535",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa7f3a5c",
   "metadata": {},
   "source": [
    "Since EDA is an iterative process. a wrangle function written to do the following\n",
    "1. Load a copy of the train data and test data\n",
    "2. For features with above between high numbers of unique features the highest\n",
    "    5 unique values are picked (high cordinality features)\n",
    "3. Very hight cordinality features are dropped\n",
    "4. Quantiles are used for highly skew feature\n",
    "5.Features with large missing values are droped(above 20%)\n",
    "6. Other NAN will be dealt with in the pipeline during modelling\n",
    "Note action will reduce both features and observation in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(datapath):\n",
    "    #read data into dataframe\n",
    "    df = pd.read_csv(datapath)\n",
    "    \n",
    "    #get list of features with above 20% missing values\n",
    "    mask = df.isna().sum()[df.isna().sum()/len(df) > 0.2].keys().tolist()\n",
    "    #drop the features above with above 20% missing value\n",
    "    df.drop(columns=mask,inplace=True)\n",
    "    \n",
    "\n",
    "    #trimming the bottom and top 10% of properties in terms of \"surface_covered_in_m2\"\n",
    "    low, high = df[\"MSSubClass\"].quantile([0.1, 0.9])\n",
    "    mask_area = df[\"MSSubClass\"].between(low, high)\n",
    "    df = df[mask_area] \n",
    "    \n",
    "    #group unique feature values in 3 classes (multicollinerity columns)\n",
    "    mid_unique_values =[]\n",
    "    large_unique_values =[]\n",
    "    target = []\n",
    "    #concatinating features with mid and low counts of unique values\n",
    "    for col in df.columns:\n",
    "        value = df[col].nunique()\n",
    "        if value < 25:\n",
    "                mid_unique_values.append(col)\n",
    "        elif col == \"SalePrice\":\n",
    "            target.append(col)\n",
    "        else:\n",
    "                large_unique_values.append(col)\n",
    "               \n",
    "    data = pd.concat([df[mid_unique_values],df[target]],axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wrangle('train_house.csv')\n",
    "test = wrangle('test_house.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023820a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d072522",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(train.info())\n",
    "print(train.columns.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57134e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "print(test.info())\n",
    "print(test.columns.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b770a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recasting float types in test as ints\n",
    "y_train = train[\"SalePrice\"]\n",
    "train.drop(columns=\"SalePrice\",inplace=True)\n",
    "\n",
    "#drop features not common to both train and test datasets\n",
    "off = []\n",
    "for col in train.columns:\n",
    "      if col not in test.columns:\n",
    "            off.append(col)\n",
    "train.drop(columns=off,inplace=True)            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "87428a85",
   "metadata": {},
   "source": [
    "#drop leaky features\n",
    "This are features that can affect the integrity of the model negatively\n",
    "No leaky feature was observed from inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cordinality of categorical features and output result as a dataframe for the train data\n",
    "categorical_data_train = train.select_dtypes(include='object').copy()\n",
    "\n",
    "count = [len(train[features].unique()) for features in categorical_data_train]\n",
    "data_tuples = list(zip(categorical_data_train,count))\n",
    "data = pd.DataFrame(data_tuples, columns=['Features','Number of distinct values'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask high cordinality feature in the train dataset (categorical)\n",
    "large_unique_value_train =[]\n",
    "for col in categorical_data_train.columns:\n",
    "    value = len(categorical_data_train[col].unique())\n",
    "    if value > 5:\n",
    "        large_unique_value_train.append(col)\n",
    "print(large_unique_value_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caste of high cordinality features for train dataset\n",
    "for val in large_unique_value_train:\n",
    "    top_5_train = train[val].value_counts().head(5)\n",
    "    train[val] = train[val].apply(lambda c: c if c in top_5_train else \"Others\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d862a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cordinality of numerical features and output result as a dataframe for train data\n",
    "int_data_train = train.select_dtypes(include='int64').copy()\n",
    "#for feature in features:\n",
    "count = [len(train[features].unique()) for features in int_data_train]\n",
    "data_tuple = list(zip(int_data_train,count))\n",
    "data = pd.DataFrame(data_tuple, columns=['Features','Number of distinct values'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74bb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating features with mid and low counts of unique values numerical train data\n",
    "large_unique_int =[]\n",
    "for col in int_data_train.columns:\n",
    "    value = len(int_data_train[col].unique())\n",
    "    if value > 5:\n",
    "        large_unique_int.append(col)\n",
    "print(large_unique_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caste of high cordinality features numerical train data\n",
    "for val in large_unique_int:\n",
    "    top_5 = train[val].value_counts().head(5)\n",
    "    train[val] = train[val].apply(lambda c: c if c in top_5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cordinality of numerical features and output result as a dataframe for test data\n",
    "categorical_data_test = test.select_dtypes(include='object').copy()\n",
    "#for feature in features:\n",
    "count = [len(test[features].unique()) for features in categorical_data_test]\n",
    "data_tuples = list(zip(categorical_data_test,count))\n",
    "data = pd.DataFrame(data_tuples, columns=['Features','Number of distinct values'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask high cordinality feature in the test dataset (categorical)\n",
    "large_unique_value_test =[]\n",
    "for col in categorical_data_test.columns:\n",
    "    value = len(categorical_data_test[col].unique())\n",
    "    if value > 5:\n",
    "        large_unique_value_test.append(col)\n",
    "print(large_unique_value_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caste of high cordinality features for test dataset\n",
    "for val in large_unique_value_test:\n",
    "    top_5_test = test[val].value_counts().head(5)\n",
    "    test[val] = test[val].apply(lambda c: c if c in top_5_test else \"Others\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cordinality of numerical features and output result as a dataframe for test data\n",
    "int_data_test = test.select_dtypes(include='int64').copy()\n",
    "#for feature in features:\n",
    "count = [len(test[features].unique()) for features in int_data_test]\n",
    "data_tuple = list(zip(int_data_test,count))\n",
    "data = pd.DataFrame(data_tuple, columns=['Features','Number of distinct values'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447821bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating features with mid and low counts of unique values numerical test data\n",
    "large_unique_int_test =[]\n",
    "for col in int_data_test.columns:\n",
    "    value = len(int_data_test[col].unique())\n",
    "    if value > 5:\n",
    "        large_unique_int_test.append(col)\n",
    "print(large_unique_int_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cceecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caste of high cordinality features numerical train data\n",
    "for val in large_unique_int_test:\n",
    "    top_5 = test[val].value_counts().head(5)\n",
    "    test[val] = test[val].apply(lambda c: c if c in top_5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical outlook of the numerical features\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e38b0bb4",
   "metadata": {},
   "source": [
    "Skimming through the above results indicates some features has outliers\n",
    "Clipping the outliers with the use of percentiles (done in the wrangle function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting columns with missing values for train dataset\n",
    "missing_col = train.isna().sum()[train.isna().sum() != 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = test.isna().sum()[test.isna().sum() != 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ded79",
   "metadata": {},
   "outputs": [],
   "source": [
    " #fill NAN with mean value.\n",
    "for col in missing_col:\n",
    "    train[col] = train[col].fillna(value='mean')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using label_encoder for all categorical features\n",
    "train_copy = train.copy()\n",
    "for c in categorical_data_train:\n",
    "    label_encoder = LabelEncoder() \n",
    "    label_encoder.fit(list(train_copy[c].values)) \n",
    "    train_copy[c] = label_encoder.transform(list(train_copy[c].values))\n",
    "    \n",
    "train=train_copy.copy()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7826653",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ba5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using label_encoder for all categorical features\n",
    "test_copy = test.copy()\n",
    "for c in categorical_data_test:\n",
    "    label_encoder = LabelEncoder() \n",
    "    label_encoder.fit(list(test_copy[c].values)) \n",
    "    test_copy[c] = label_encoder.transform(list(test_copy[c].values))\n",
    "    \n",
    "test=test_copy.copy().dropna()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd198be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train = clean_train(train)\n",
    "#test = clean_train(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model = Ridge()\n",
    "\n",
    "# Fit model\n",
    "model.fit(train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf24846",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pd.Series(model.predict(test))\n",
    "y_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model.coef_\n",
    "features = train.columns\n",
    "feat_imp = pd.Series(coefficients,index=features).sort_values(key=abs).tail(15)\n",
    "feat_imp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93869f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bar chart for the 15 most influential coefficients of the model\n",
    "feat_imp.plot(kind=\"barh\")\n",
    "# Label axes\n",
    "plt.xlabel(\"Importance [USD]\")\n",
    "plt.ylabel(\"Feature\")\n",
    "# Add title\n",
    "plt.title(\"Feature Importances for Apartment Price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model for future use,using pickle\n",
    "import pickle \n",
    "\n",
    "pickle.dump(model, open('model_house.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
