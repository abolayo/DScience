{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M16jEgLwqhR-"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the DSN Internship Coding Challenge! This assessment will put your Natural Language Processing (NLP) and problem-solving abilities to the test. :\n",
    "\n",
    "- Section one of the assessmnet will require you to build a text classification model.\n",
    "\n",
    "Good luck! If you have questions about the framing of the questions, please contact **recruitment@datasciencenigeria.ai**\n",
    "\n",
    "### How to Use and Submit this Notebook.\n",
    "- Make a copy of this document and rename it **Firstname_Lastname_DSNInternshipCodingAssessment.ipynb**\n",
    "- Before attempting to submit, ensure that you have ran all of the cells in your notebook and the output visible.\n",
    "- Once youâ€™ve completed all tasks, save and download a copy of the notebook as .ipynb\n",
    "- Submit a link (make sure that the link is set to \"Anyone on the internet with the link can view\"), the downloaded copy of your final notebook via this [link](https://forms.gle/t8sFNrfAymZUrfJq7).\n",
    "\n",
    "### What Not to Do.\n",
    "- Do not share this document with any external party\n",
    "- No teamwork is permitted\n",
    "- After submitting a copy of your script, you are not permitted to make any changes to the online version; any discrepancy between the online and submitted copies will render your application null and void."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpRXFFAUphKP"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "This is a news [dataset](https://drive.google.com/file/d/1NgPM7_mFCDKnuqI9SamMCrkF1mE5AgAI/view?usp=sharing) which contains 2225 examples of news articles with their respective labels. Use to the link to learn more about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8nu90TBySAA"
   },
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSP58peMray5"
   },
   "source": [
    "### Task\n",
    "\n",
    "**This is to test your knowledge on NLP**\n",
    "\n",
    "Build and train a machine learning model with the provided dataset to classify the news category or topic. You can use any architecture or model, in this test.\n",
    "\n",
    "**Make sure to plot the accuracy vs epochs and loss vs epochs graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qChXbLcxcoKp",
    "outputId": "cb26a878-7281-4e5b-f89b-255780faf338"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FeQmjMYLpUUF",
    "outputId": "12e2ba9c-67ef-4448-bd19-943b2dd33255"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwordnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from markupsafe import escape\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "T5_r7Qilp46S",
    "outputId": "2e11336f-2d39-4401-ce85-24d9810cd2af"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/bbc-text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEjbqqahYZuu"
   },
   "source": [
    "A quick look at our data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "Uug92XeTYp93",
    "outputId": "8910eae7-a13a-4e72-8a26-bef9ac9aee6b"
   },
   "outputs": [],
   "source": [
    "df[df[\"category\"] == 'sport'][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnM1XdpAa4x9",
    "outputId": "a278850b-3717-4a56-850e-30b5341e50cf"
   },
   "outputs": [],
   "source": [
    "# shape of the dataset\n",
    "print(df.shape)\n",
    "# total number of unique categories\n",
    "print(\"Unique categories:\",df['category'].nunique())\n",
    "print(\"-------------------------------------------------\")\n",
    "# information about metadata\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "7JXCvHc1cRXh",
    "outputId": "71ec7c13-238d-4d73-d2c0-62957abfd916"
   },
   "outputs": [],
   "source": [
    "# descibtion of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSe6x4F4dNit",
    "outputId": "575d8881-30ef-4d6c-c3d3-43f4975c3c88"
   },
   "outputs": [],
   "source": [
    "\n",
    "# model building imports\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from keras.layers import Conv1D, SimpleRNN, Bidirectional, MaxPooling1D, GlobalMaxPool1D, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# matplotlib defaults\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "PUDhIa3xeJ5f",
    "outputId": "fa4d16b6-4c26-418c-e757-ec5463ce3931"
   },
   "outputs": [],
   "source": [
    "# Top categories by and number of articles per categories\n",
    "cat_df = pd.DataFrame(df['category'].value_counts()).reset_index()\n",
    "sns.barplot(cat_df,x='index',y='category')\n",
    "plt.title(\"Categories of text\", size=15)\n",
    "plt.xlabel(\"Categories of text\", size=14)\n",
    "plt.ylabel(\"Number of text\", size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jobXKyhAfBWj",
    "outputId": "41483950-e318-40d9-e37b-94069b75b142"
   },
   "outputs": [],
   "source": [
    "final_df = df.copy()\n",
    "final_df['length_of_text'] = final_df['text'].map(lambda x: len(x))\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iVKNDvk7iTYk",
    "outputId": "9ecc6698-ca4e-4042-f7d0-f88e8b731b7c"
   },
   "outputs": [],
   "source": [
    "# maximum length of text in each category\n",
    "lenmax_df = final_df.groupby('category')['length_of_text'].max().reset_index().sort_values(by='length_of_text',ascending=False)\n",
    "lenmax_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "m8C6pvtUkVO0",
    "outputId": "dd672f18-7cc6-4339-fac1-48e3cad58a83"
   },
   "outputs": [],
   "source": [
    "# wordcloud of categories of text in our dataset\n",
    "plt.figure(figsize=(6,6))\n",
    "wc = WordCloud(max_words=1000,\n",
    "               min_font_size=10,\n",
    "               height=600,\n",
    "               width=1600,\n",
    "               background_color='black',\n",
    "               contour_color='black',\n",
    "               colormap='plasma',\n",
    "               repeat=True,\n",
    "               stopwords=STOPWORDS).generate(' '.join(final_df.category))\n",
    "\n",
    "plt.title(\"Text Wordcloud\", size=10, weight='bold')\n",
    "plt.imshow(wc, interpolation= \"bilinear\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "mydJNPUUmhca",
    "outputId": "4f82c133-c191-4039-c771-2ef79418e14c"
   },
   "outputs": [],
   "source": [
    "# create new dataframe of category and length of each text in that categories\n",
    "new_df = final_df.copy()\n",
    "new_df.drop('length_of_text', inplace=True, axis=1)\n",
    "\n",
    "# list of categories in out dataset\n",
    "categories = cat_df['index'].to_list()\n",
    "\n",
    "# list of news articles of each top 10 categories list\n",
    "text_list = []\n",
    "\n",
    "for i in categories:\n",
    "    cat_ndf = new_df[new_df['category'] == i]\n",
    "    cat_array = cat_ndf['text'].values  # array of news articles text in each category\n",
    "    text_list.append(cat_array)\n",
    "\n",
    "# create a wordcloud instance\n",
    "wc1 = WordCloud(max_words=1000,\n",
    "               min_font_size=10,\n",
    "               height=600,\n",
    "               width=1600,\n",
    "               background_color='black',\n",
    "               contour_color='black',\n",
    "               colormap='plasma',\n",
    "               repeat=True,\n",
    "               stopwords=STOPWORDS)\n",
    "\n",
    "# plot the figure of 10 wordcloud from out dataset\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "for idx, j in enumerate(categories):\n",
    "    plt.subplot(5,2,idx+1)\n",
    "    cloud = wc1.generate(' '.join(text_list[idx]))\n",
    "    plt.imshow(cloud, interpolation= \"bilinear\")\n",
    "    plt.title(f\"Wordcloud for {j}\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0egMjyDl-kR"
   },
   "outputs": [],
   "source": [
    "# start with removing some data from politics dataframe ndf\n",
    "politics_list = list(new_df[new_df['category'] == 'politics'].index)\n",
    "list_1 = politics_list\n",
    "\n",
    "# drop the  labels from the dataset\n",
    "ndf2 = new_df.copy()\n",
    "ndf2.drop(list_1, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8VjyHSuFTExt",
    "outputId": "c4b6b00d-1bc5-49bd-a01b-00754a9b8093"
   },
   "outputs": [],
   "source": [
    "ndf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4IsS3qoSoF1"
   },
   "outputs": [],
   "source": [
    "# clean the text data using regex and data cleaning function\n",
    "def datacleaning(text):\n",
    "    whitespace = re.compile(r\"\\s+\")\n",
    "    user = re.compile(r\"(?i)@[a-z0-9_]+\")\n",
    "    text = whitespace.sub(' ', text)\n",
    "    text = user.sub('', text)\n",
    "    text = re.sub(r\"\\[[^()]*\\]\",\"\", text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\"(?:@\\S*|#\\S*|http(?=.*://)\\S*)\", \"\", text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # removing stop-words\n",
    "    text = [word for word in text.split() if word not in list(STOPWORDS)]\n",
    "\n",
    "    # word lemmatization\n",
    "    sentence = []\n",
    "    for word in text:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence.append(lemmatizer.lemmatize(word,'v'))\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZEQpQYRrzoJ",
    "outputId": "3bd3fe42-2748-4a7b-a8e9-27ce2e5d7578"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j2SR9WRip-L"
   },
   "outputs": [],
   "source": [
    "\n",
    "new_df.text = new_df.text.apply(lambda x: datacleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGhmj-L00e9g"
   },
   "outputs": [],
   "source": [
    "final_df.text = final_df.text.apply(lambda x: datacleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uaKNNIzXf1B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQgomLSxc_Ob"
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imRG8e9rdDZ0",
    "outputId": "64830d30-c10c-45c9-d196-1b17905a9859"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(final_df, test_size=1-TRAIN_SIZE,\n",
    "                                         random_state=42) # Splits Dataset into Training and Testing set\n",
    "print(\"Train Data size:\", len(train_data))\n",
    "print(\"Test Data size\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "E9aefidydT19",
    "outputId": "19a58e7a-b2b2-4b43-eb7f-ee32948f522c"
   },
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VefapvvOaqDP",
    "outputId": "e0e76fc1-e63c-425b-8fa4-627fd26a29cb"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data.text)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rs_1jSIW3GD1",
    "outputId": "5dd2c177-f639-4310-c912-7accf627087b"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow\n",
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5r6xefrQ9MvM",
    "outputId": "41f8131b-1f03-4996-be45-7dd890666009"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3X86sSXbtm2"
   },
   "outputs": [],
   "source": [
    "labels = train_data.category.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2oEYKgOcC9P",
    "outputId": "f9f061bf-dc35-4802-8a28-075fc2a89d5f"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data.category.to_list())\n",
    "\n",
    "y_train = encoder.transform(train_data.category.to_list())\n",
    "y_test = encoder.transform(test_data.category.to_list())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HAzB095jm_G"
   },
   "source": [
    "Word Emdedding\n",
    "In Language Model, words are represented in a way to intend more meaning and for learning the patterns and contextual meaning behind it.\n",
    "\n",
    "Word Embedding is one of the popular representation of document vocabulary.It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "\n",
    "Basically, it's a feature vector representation of words which are used for other natural language processing applications.\n",
    "\n",
    "We could train the embedding ourselves but that would take a while to train and it wouldn't be effective. So going in the path of Computer Vision, here we use Transfer Learning. We download the pre-trained embedding and use it in our model.\n",
    "\n",
    "The pretrained Word Embedding like GloVe & Word2Vec gives more insights for a word which can be used for classification. If you want to learn more about the Word Embedding, please refer some links that I left at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFA8eiD4jqRC",
    "outputId": "fee3c922-296c-4154-f907-b42eb89f7809"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvqZgikQkFYn"
   },
   "outputs": [],
   "source": [
    "GLOVE_EMB = 'glove.6B.300d.txt'\n",
    "EMBEDDING_DIM = 300\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "MODEL_PATH = '.../best_model.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtAkh2U-lKV7",
    "outputId": "d754be83-0c5a-4658-9642-1ece2c804c64"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open(GLOVE_EMB)\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = value = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' %len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJDlnnftlQWh"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I2znI0LlW6_"
   },
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                          trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgvVLmGHi2YX"
   },
   "outputs": [],
   "source": [
    "!pip install keras==2.12.0rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9VghUn2lbM4"
   },
   "source": [
    "Model Training - LSTM\n",
    "We are clear to build our Deep Learning model. While developing a DL model, we should keep in mind of key things like Model Architecture, Hyperparmeter Tuning and Performance of the model.\n",
    "\n",
    "As you can see in the word cloud, the some words are predominantly feature in all categories. This could be a problem if we are using a Machine Learning model like Naive Bayes, SVD, etc.. That's why we use Sequence Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYLvGeUvlhQZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "nxb-zOvhlnab",
    "outputId": "e0e1cc3a-4001-4c3a-a673-1f1b8bf58f3b"
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')#(x)\n",
    "model = tf.keras.Model(sequence_input, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "sMFbiYu2hHQS",
    "outputId": "d37415df-ab02-4de3-db7d-ba0278047db5"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "# This builds the model for the first time:\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f44sEQ9Pl1iX",
    "outputId": "c2bd6532-07c9-4411-ca45-31076d3947f1"
   },
   "outputs": [],
   "source": [
    "print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "oqkQ9D0cHhsc",
    "outputId": "db367f36-0eb6-45fc-9760-93593183dc95"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "id": "QJ1HcZgLl_7w",
    "outputId": "635ca74c-8a74-4f5c-bd41-6abfeafa24f6"
   },
   "outputs": [],
   "source": [
    "s, (at, al) = plt.subplots(2,1)\n",
    "at.plot(history.history['accuracy'], c= 'b')\n",
    "at.plot(history.history['val_accuracy'], c='r')\n",
    "at.set_title('model accuracy')\n",
    "at.set_ylabel('accuracy')\n",
    "at.set_xlabel('epoch')\n",
    "at.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n",
    "\n",
    "al.plot(history.history['loss'], c='m')\n",
    "al.plot(history.history['val_loss'], c='c')\n",
    "al.set_title('model loss')\n",
    "al.set_ylabel('loss')\n",
    "al.set_xlabel('epoch')\n",
    "al.legend(['train', 'val'], loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbzUedEsmCD5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
